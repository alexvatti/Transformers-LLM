{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer learning in NLP (Transfomers) Encoder\n",
        "# bert-base-uncased\n",
        "\n",
        "\n",
        " bert-base-uncased\t110M\tEnglish"
      ],
      "metadata": {
        "id": "XAEKJ1SUYzZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Define function to create embeddings\n",
        "def bert_embeddings(texts, max_length=32):\n",
        "    inputs = tokenizer(\n",
        "        texts.tolist(),\n",
        "        return_tensors=\"tf\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "    outputs = bert_model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "    cls_embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token's embedding\n",
        "    return cls_embeddings\n",
        "\n",
        "# Load your dataset\n",
        "file_path = \"https://raw.githubusercontent.com/alexvatti/full-stack-data-science/main/NLP-Exercises/Email-Spam-Classification/spam.csv\"\n",
        "email_df = pd.read_csv(file_path)\n",
        "X = email_df['Message']  # Column with email text\n",
        "y = email_df['Category'].apply(lambda x: 1 if x == 'spam' else 0)  # Label encoding for spam/ham\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert emails to BERT embeddings\n",
        "X_train_embeddings = bert_embeddings(X_train)\n",
        "X_test_embeddings = bert_embeddings(X_test)\n",
        "\n",
        "# Define a simple classifier model\n",
        "classifier = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(768,)),\n",
        "    Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the classifier\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the classifier\n",
        "classifier.fit(X_train_embeddings, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = classifier.evaluate(X_test_embeddings, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predictions and confusion matrix\n",
        "y_pred = (classifier.predict(X_test_embeddings) > 0.5).astype(\"int32\")\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# Save the trained model to a file\n",
        "classifier.save(\"spam_classification_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjm7mYeTn7z-",
        "outputId": "de5dcda1-96ef-4c5b-8dc2-ce6391d8d72d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8990 - loss: 0.2445 - val_accuracy: 0.9664 - val_loss: 0.1067\n",
            "Epoch 2/5\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9817 - loss: 0.0596 - val_accuracy: 0.9709 - val_loss: 0.1280\n",
            "Epoch 3/5\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9864 - loss: 0.0436 - val_accuracy: 0.9709 - val_loss: 0.0968\n",
            "Epoch 4/5\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9866 - loss: 0.0345 - val_accuracy: 0.9709 - val_loss: 0.0968\n",
            "Epoch 5/5\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9906 - loss: 0.0300 - val_accuracy: 0.9709 - val_loss: 0.1046\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9818 - loss: 0.0638\n",
            "Test Accuracy: 0.9811659455299377\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[957   9]\n",
            " [ 12 137]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       966\n",
            "           1       0.94      0.92      0.93       149\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.96      0.96      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "elSVF6iWYx5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "\n",
        "# Function to tokenize and encode the input text\n",
        "# Load the tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Define function to create embeddings\n",
        "def bert_embeddings(texts, max_length=32):\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"tf\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "    outputs = bert_model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "    cls_embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token's embedding\n",
        "    return cls_embeddings\n",
        "\n",
        "# Function to make predictions with the trained model\n",
        "def predict_spam_or_ham(text, model, tokenizer):\n",
        "    # Preprocess the input text\n",
        "    cls_embeddings = bert_embeddings([text])\n",
        "    print(cls_embeddings.shape)\n",
        "\n",
        "    # Make a prediction (assuming binary classification: spam or ham)\n",
        "    prediction = model.predict(cls_embeddings)\n",
        "\n",
        "    # Assuming a binary classification output: 0 or 1\n",
        "    predicted_class = \"Spam\" if prediction[0] > 0.5 else \"Ham\"\n",
        "    return predicted_class\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = load_model(\"spam_classification_model.h5\")\n",
        "\n",
        "# Example text input to classify\n",
        "text_input = \"Congratulations! You've won a free gift card. Click here to claim.\"\n",
        "\n",
        "# Test the saved model on the input text\n",
        "result = predict_spam_or_ham(text_input, loaded_model, tokenizer)\n",
        "print(f\"Prediction: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtX0Iw2WxUue",
        "outputId": "5f9ccea9-fdd1-49c7-bbc8-4b58d1f6e304"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 768)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
            "Prediction: Spam\n"
          ]
        }
      ]
    }
  ]
}